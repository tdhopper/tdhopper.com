---
title: Rosetta Stone for Stochastic Dynamic Programming
categories:
  - "Articles"
slug: rosetta-stone-for-stochastic-dynamic-programming
aliases: [/blog/2014/Jan/22/rosetta-stone-for-stochastic-dynamic-programming/]
date: 2014-01-22
---


In grad school, I worked on what operations researchers call [approximate dynamic programming](http://adp.princeton.edu/). This field is virtually identical to what computer scientists call [reinforcement learning](http://en.wikipedia.org/wiki/Reinforcement_learning). It's also been called [neuro-dynamic programming](http://athenasc.com/NDP_Review.pdf). All these things are an extension of stochastic dynamic programming, which is usually introduced through [Markov decision processes](http://en.wikipedia.org/wiki/Markov_decision_process).

Because these overlapping fields have been developed in different disciplines, mathematical notation for them can vary dramatically. After much confusion and frustration, I created this Rosetta Stone to help me keep everything straight. I hope others will find it useful as well. A PDF and the original LaTeX code are available on [my Github](https://github.com/tdhopper/Rosetta-Stone-for-Stochastic-Dynamic-Programming).

|                   |      Bertsekas       |     Sutton and Barto     |          Puterman          |                Powell                |
| ----------------- | -------------------- | ------------------------ | -------------------------- | ------------------------------------ |
| Stages            | $k$                  | $t$                      | $t$                        | $t$                                  |
| First Stage       | $N$                  | $1$                      | $1$                        | 1                                    |
| Final Stage       | $0$                  | $T$                      | $N$                        | $T$                                  |
| State Space       |                      |                          | $S$                        | $\mathcal{S}$                        |
| State             | $i$, $i_{k}$         | $s$                      | $s$                        | $s$, $S_{t}$                         |
| Action Space      | $U(i)$               |                          | $A=\cup_{s\in S}A_{s}$     | $\mathcal{A}$                        |
| Action            |                      | $a$                      | $a$                        | $a$                                  |
| Policy            | $\mu_{k}(i)$, $\pi$  | $\pi(s,a)$, $\pi$        | $\pi$, $d_{t}^{MD}(s)$     | $\pi$                                |
| Transitions       | $p_{ij}(\mu_{k}(i))$ | $\mathcal{P}_{ss'}^{a}$  | $p_{t}(\cdot\,\vert\,s,a)$ | $\mathbb{P}(s'\,\vert\,S_{t},a_{t})$ |
| Cost              | $g(i,u,j)$           | $\mathcal{R}_{ss'}^{a}$  | $r_t(s,a)$                 | $C_{t}(S_{t},a_{t})$                 |
| Terminal Cost     | $G(i_{N})$           | $r_{T}$                  | $r_{N}(s)$                 | $V_{T}(S_{T})$                       |
| Discount          | $\alpha$             | $\gamma$                 | $\lambda$                  | $\gamma$                             |
| Q-Value (Policy)  | $J_{k}^{\pi}(i)$     | $\mathcal{Q}^{\pi}(s,a)$ |                                                                  ||
| Q-Value (Optimal) |                      |                          |                            | $\mathcal{Q}(S^{n},a)$               |
| Value (Policy)    | $J_{k}^{\pi}(i)$     | $V^{\pi}(s)$             | $u_{t}^{\pi}$              | $V_{t}^{\pi}(S_{t})$                 |
| Value (Optimal)   | $J_{k}^{*}(i)$       | $V^{*}(s)$               | $u_{t}^{*}$                | $V_{t}(S_{t})$                       |
| Bellman Operator  | $T$                  |                          | $\mathscr{L}$, $L$         | $\mathcal{M}$                        |


### Optimal Value Function

* Bertsekas

$$J_{k}^{\*}=\min_{u\in U(i)}\sum_{j=1}^{n}p_{ij}(u)\left(g(i,u,j)+\alpha J^{\*}_{k-1}(j)\right)$$

* Sutton and Barto

$$V^{\*}(s)=\max\_{a}\mathcal{P}^{a}\_{ss'}\left[\mathcal{R}^{a}_{ss'}+\gamma V^{\*}(s')\right]$$

* Puterman

$$u\_{t}^{\*}(s\_{t})=\max\_{a\in A\_{s\_{t}}}\left\\{r\_{t}(s\_{t},a)+\sum\_{j\in S}p\_{t}(j\,|\,s\_{t},a)u\_{t+1}^{\*}(j)\right\\}$$

* Powell

$$V\_{t}(S\_{t})=\max\_{a\_{t}}\left\\{C\_{t}(S\_{t},a\_{t})+\gamma\sum\_{s'\in \mathcal{S}}\mathbb{P}(s'\,\vert\,S\_{t},a\_{t})V\_{t+1}(s')\right\\}$$

### References

* D.P. Bertsekas. _Dynamic Programming and Optimal Control_. Number v. 2 in Athena Scientific Optimization and Computation Series. Athena Scientific, 2007. ISBN 9781886529304. [URL](http://books.google.com/books?id=eL01YAAACAAJ).
* W.B. Powell. _Approximate Dynamic Programming: Solving the Curses of Dimensionality_. Wiley Series in Probability and Statistics. John Wiley & Sons, 2011. ISBN 9781118029152. [URL](http://books.google.com/books?id=VBuZhne7pmwC).
* M.L. Puterman. _Markov decision processes: discrete stochastic dynamic programming_. Wiley series in probability and statistics. Wiley-Interscience, 1994. ISBN 9780471727828. [URL](http://books.google.com/books?id=Y-gmAQAAIAAJ).
* R.S. Sutton and A.G. Barto. _Reinforcement Learning: An Introduction_. Adaptive Computation and Machine Learning. Mit Press, 1998. ISBN 9780262193986. [URL](http://books.google.com/books?id=CAFR6IBF4xYC).



